{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import libraries\n",
    "- numpy - for fast computations\n",
    "- scipy - for loading matlab files (.mat extension)\n",
    "- matplotlib - for plotting images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from numpy.linalg import svd\n",
    "from scipy.io import loadmat\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define Main Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- featureNormalize - which normalizes matrix values to be normally distributed (Mean=0, STD=1)\n",
    "- pca - does Singular Value Decomposition and calculates pca for given matrix\n",
    "- projectData - projects data on basis which consists of top K meaningful Eigenvectors\n",
    "- recoverData - recovers data after projection and generates image approximations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def featureNormalize(X):\n",
    "    \"\"\"\n",
    "    Returns a normalized version of X where the mean value of each feature is 0 and the standard deviation is 1.\n",
    "    \"\"\"\n",
    "    # calculates mean and standard deviation\n",
    "    mu = np.mean(X,axis=0)\n",
    "    sigma = np.std(X,axis=0)\n",
    "    \n",
    "    # normalized each matrix element to have them normally distributed N(0,1) (mean=0, std=1)\n",
    "    X_norm = (X - mu)/sigma\n",
    "    \n",
    "    return X_norm, mu , sigma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def covariance(X):\n",
    "    m, n = X.shape[0], X.shape[1]\n",
    "    \n",
    "    X_cov = 1/(m - 1) * X.T @ X\n",
    "    \n",
    "    return X_cov"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pca(X):\n",
    "    \"\"\"\n",
    "    Computes eigenvectors of the covariance matrix of X\n",
    "    \"\"\"\n",
    "    sigma = covariance(X)\n",
    "    \n",
    "    U,S,V = svd(sigma)\n",
    "    \n",
    "    return U,S,V"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def projectData(X, U, K):\n",
    "    \"\"\"\n",
    "    Computes the reduced data representation when projecting only on to the top k eigenvectors\n",
    "    \"\"\"\n",
    "    m = X.shape[0]\n",
    "    U_reduced = U[:,:K]\n",
    "    Z = np.zeros((m,K))\n",
    "    \n",
    "    for i in range(m):\n",
    "        for j in range(K):\n",
    "            Z[i,j] = X[i,:] @ U_reduced[:,j]\n",
    "    \n",
    "    return Z     # Project the data onto K=1 dimension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def recoverData(Z, U, K):\n",
    "    \"\"\"\n",
    "    Recovers an approximation of the original data when using the projected data\n",
    "    \"\"\"\n",
    "    m,n = Z.shape[0],U.shape[0]\n",
    "    X_rec = np.zeros((m,n))\n",
    "    U_reduced = U[:,:K]\n",
    "    \n",
    "    for i in range(m):\n",
    "        X_rec[i,:] = Z[i,:] @ U_reduced.T\n",
    "    \n",
    "    return X_rec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_flattened_vectors(matrix, k:int, title:str):\n",
    "    \"\"\"Shows first K vectors as an reshaped 32x32 dimensional images from X matrix\"\"\"\n",
    "\n",
    "    # calculate dimension of plot matrix which fits all K feature representations (as images) which will look like as eigenfaces\n",
    "    PLOT_MATRIX_DIM = 2 if k==1 else int(np.ceil(np.sqrt(k)))\n",
    "\n",
    "    # take matrix of only that first K features\n",
    "    reduced_matrix = matrix[:,:k].T\n",
    "\n",
    "    # plotting compressed images\n",
    "    fig2, ax2 = plt.subplots(PLOT_MATRIX_DIM, PLOT_MATRIX_DIM, figsize=(18,18))\n",
    "\n",
    "    for i in range(0, PLOT_MATRIX_DIM**2, PLOT_MATRIX_DIM):\n",
    "        for j in range(PLOT_MATRIX_DIM):\n",
    "            if i + j < k:\n",
    "                reshaped_image = reduced_matrix[i+j,:].reshape(32,32,order=\"F\")\n",
    "                ax2[int(i/PLOT_MATRIX_DIM),j].imshow(reshaped_image,cmap=\"gray\")\n",
    "                ax2[int(i/PLOT_MATRIX_DIM),j].axis(\"off\")\n",
    "    _ = fig2.suptitle(title, fontsize=30)\n",
    "                "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load and visualize the Face image dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "resources = Path.cwd() / \"resources\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mat = loadmat(resources / \"principal-component-analysis\" / \"ex7faces.mat\")\n",
    "X = mat[\"X\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each image size is 32x32 and they are flattened in 1024 size vector. We have 5K images in total."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## create subplots to show multiple images\n",
    "1. unflatten image vectors\n",
    "2. plot images\n",
    "3. turn off axis to have less noise on plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_flattened_vectors(X.T, k=16, title='Original Images')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Normalize matrix of flattened images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_norm, _ , _ = featureNormalize(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_flattened_vectors(X_norm.T, k=16, title='Normalized Images')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run PCA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**U** and **V** are **left singular** and **right singular** vectors of covariance matrix of **X normalized** matrix\n",
    "\n",
    "**S** is known as singular values vector which in fact is a representation of diagonal matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "U, S, V = pca(X_norm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Understanding U, S, V matrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_norm.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "U.shape, S.shape, V.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## $U$ and $V^T$ are equal (Almost) because they are containing left and right singular vectors of covariance matrix which is symmetric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "U[1,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "V[:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.linalg.norm(U-V.T)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Show Singular Values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## All singular values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.bar(range(S.shape[0]), S)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "U.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "V.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Top 50 singular values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.bar(range(50), S[:50])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Below is the visualization of 36 principal componentsÂ or eigenvectors or eigenfaces that describe the largest variations in the dataset\n",
    "\n",
    "These vectors are looking as if they were reduces form of original images but they are eigenfaces. If we want to map our n=1024 dimensional images into lower K dimensional (K<<n) space, we are saving coefficients for first K eigenvectors/eigenfaces and then we can get reconstruction (approximate) of original images with linear combination of these first K important eigenfaces with saved coefficients. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_flattened_vectors(U, k=36, title='First 36 Important Feature Vectors (EigenFaces)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Let's try to take 36 last principal components instead of first. We will see that these last eigenfaces aren't impressive since they contain almost random noise."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I am just passing matrix of last 36 feature vectors as a parameter to avoid problems with indices since it takes first K element and it's impossible to access slice of [:, -k:] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_flattened_vectors(U[:,-36:], k=36, title='Last 36 Important Feature Vectors (EigenFaces)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Let's project the data onto the first K=100 principal components and recover back. Try to understand what is lost in the dimensionality reduction process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "K = 100\n",
    "Z = projectData(X_norm, U, K)\n",
    "print(\"The projected data Z has a size of:\", Z.shape)\n",
    "\n",
    "### Data reconstruction\n",
    "X_rec  = recoverData(Z, U, K)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize the reconstructed data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "show_flattened_vectors(X_norm.T, k=16, title='Normalized Images')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_flattened_vectors(X_rec.T, k=16, title=f\"Reconstructed images from {K} dimensional space\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Let's project the data onto the first K=1 principal components and recover back. Try to understand what is lost in the dimensionality reduction process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "K = 1\n",
    "Z = projectData(X_norm, U, K)\n",
    "print(\"The projected data Z has a size of:\", Z.shape)\n",
    "\n",
    "### Data reconstruction\n",
    "X_rec  = recoverData(Z, U, K)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize the reconstructed data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_flattened_vectors(X_norm.T, k=16, title='Normalized Images')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_flattened_vectors(X_rec.T, k=16, title=f\"Reconstructed images from {K} dimensional space\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we check first feature with highest singular value in V matrix we will see that all of these representations of images come from that feature with little scaling. They are multiplied with the coefficient to have their reconstruction as close as possible with original images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_flattened_vectors(U, k=1, title='Feature vector with Highest highest singular value')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
