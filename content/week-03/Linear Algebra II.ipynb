{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create anaconda environment\n",
    "<br>\n",
    "```bash\n",
    "conda create -n ml python=3.7.4 jupyter\n",
    "```\n",
    "Install fastai library\n",
    "<br>\n",
    "```bash\n",
    "conda install -c pytorch -c fastai fastai\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Special types of matrices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Identity matrix $I_{n} \\in \\mathbb{R}^{n \\times n}$ is a matrix which does not change other matrix after multiplication. These kind of matrices contain ones on main diagonal and zero everywhere else \n",
    "$$\\begin{align} I_{n} &= \\begin{pmatrix}\n",
    "           1, 0, \\dots, 0 \\\\\n",
    "           0, 1, \\dots, 0 \\\\\n",
    "           \\vdots \\\\\n",
    "           0, 0, \\dots, 1 \\\\\n",
    "         \\end{pmatrix}\n",
    "  \\end{align}$$\n",
    "<br>\n",
    "or we can define it with property $\\forall a \\in \\mathbb{R}^{1 \\times n}$ holds $aI_{n} = a$ or $\\forall a \\in \\mathbb{R}^{n \\times 1}$ holds $I_{n}a =a$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "I = np.identity(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 0., 0., 0.],\n",
       "       [0., 1., 0., 0.],\n",
       "       [0., 0., 1., 0.],\n",
       "       [0., 0., 0., 1.]])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "I"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "A = np.random.random(size=(4, 5))\n",
    "B = np.random.random(size=(6, 4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.08180764, 0.13233535, 0.50190654, 0.94090784, 0.23366253],\n",
       "       [0.65533627, 0.89712887, 0.06492877, 0.35594497, 0.26581298],\n",
       "       [0.67741343, 0.10612077, 0.76232457, 0.07079163, 0.9766638 ],\n",
       "       [0.78773022, 0.2730207 , 0.96932571, 0.89913794, 0.16359123]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.59625685, 0.21514496, 0.22955471, 0.59314179],\n",
       "       [0.75225961, 0.04911663, 0.82749338, 0.11810451],\n",
       "       [0.35309953, 0.21511622, 0.36902663, 0.83973281],\n",
       "       [0.34193753, 0.75561623, 0.1429398 , 0.79237438],\n",
       "       [0.88373539, 0.16254689, 0.46374738, 0.66354648],\n",
       "       [0.05174316, 0.53756595, 0.1425859 , 0.76509128]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.08180764, 0.13233535, 0.50190654, 0.94090784, 0.23366253],\n",
       "       [0.65533627, 0.89712887, 0.06492877, 0.35594497, 0.26581298],\n",
       "       [0.67741343, 0.10612077, 0.76232457, 0.07079163, 0.9766638 ],\n",
       "       [0.78773022, 0.2730207 , 0.96932571, 0.89913794, 0.16359123]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "I @ A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.59625685, 0.21514496, 0.22955471, 0.59314179],\n",
       "       [0.75225961, 0.04911663, 0.82749338, 0.11810451],\n",
       "       [0.35309953, 0.21511622, 0.36902663, 0.83973281],\n",
       "       [0.34193753, 0.75561623, 0.1429398 , 0.79237438],\n",
       "       [0.88373539, 0.16254689, 0.46374738, 0.66354648],\n",
       "       [0.05174316, 0.53756595, 0.1425859 , 0.76509128]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "B @ I"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inverse matrix of $A \\in \\mathbb{R}^{n \\times n}$, is the matrix $A^{-1} \\in \\mathbb{R}^{n \\times n}$ for which $A^{-1}A = I$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.81890884, 0.78825974, 0.87439842, 0.94759203, 0.5461928 ,\n",
       "        0.13407198, 0.96945139, 0.9188376 ],\n",
       "       [0.87818807, 0.56492261, 0.16592348, 0.42610048, 0.74483667,\n",
       "        0.51242792, 0.68651341, 0.32431056],\n",
       "       [0.96894838, 0.61132545, 0.16142936, 0.84775417, 0.47020687,\n",
       "        0.15378486, 0.10566062, 0.65394892],\n",
       "       [0.1161688 , 0.12452046, 0.11319474, 0.90558559, 0.26105095,\n",
       "        0.5540081 , 0.36826309, 0.57233497],\n",
       "       [0.49719397, 0.87143557, 0.82084637, 0.91544766, 0.28174064,\n",
       "        0.95373883, 0.24207103, 0.71282905],\n",
       "       [0.19356225, 0.79483475, 0.47494288, 0.32012374, 0.48439538,\n",
       "        0.47473527, 0.70758254, 0.03913135],\n",
       "       [0.84760828, 0.23620354, 0.41211923, 0.86124438, 0.88559007,\n",
       "        0.2094045 , 0.40099728, 0.30124624],\n",
       "       [0.94832213, 0.03883361, 0.77862773, 0.28588957, 0.65135352,\n",
       "        0.779747  , 0.31322503, 0.42284284]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A = np.random.random(size=(8, 8))\n",
    "A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  3.46384409,  19.4941747 , -19.57516426,  -9.99832219,\n",
       "         16.87962614, -21.72840338,  10.68560741, -12.72897264],\n",
       "       [ -1.10586623,  -5.12078341,   6.11757728,   2.0388313 ,\n",
       "         -4.27217314,   6.48526459,  -3.30193302,   3.0640342 ],\n",
       "       [  0.68791074,   0.38077008,  -1.9615242 ,  -1.32420149,\n",
       "          1.6404582 ,  -1.4065594 ,   1.33985945,  -0.55078586],\n",
       "       [  2.58608377,  14.65423917, -16.01834189,  -7.15281545,\n",
       "         13.78490596, -16.9852474 ,   9.6480159 , -10.94445835],\n",
       "       [ -5.82030521, -30.09706056,  31.5845337 ,  15.0318286 ,\n",
       "        -27.0470216 ,  34.0190968 , -16.34500627,  20.6302604 ],\n",
       "       [  0.05003706,   4.15891093,  -4.04486993,  -1.3253622 ,\n",
       "          3.61229059,  -4.0262299 ,   1.62408924,  -2.12303422],\n",
       "       [  2.82978224,  12.70682825, -13.32494727,  -5.54530857,\n",
       "         10.17134655, -13.3880142 ,   6.46532306,  -8.29540923],\n",
       "       [ -3.90488946, -24.57896545,  26.45841824,  12.90733087,\n",
       "        -22.33715585,  27.14770809, -15.25813491,  17.32584407]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "invA = np.linalg.inv(A)\n",
    "invA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 1.00000000e+00 -3.98888673e-15 -4.35219105e-15 -4.70036671e-15\n",
      "  -6.60232898e-15 -1.28837269e-15 -4.56649980e-15  3.96398639e-16]\n",
      " [ 1.42512459e-15  1.00000000e+00  2.43207058e-15  6.79481724e-16\n",
      "   7.88674531e-16  1.83800676e-15  9.83902242e-16 -1.77459679e-16]\n",
      " [ 4.04279473e-16  2.17604780e-15  1.00000000e+00  8.57498938e-16\n",
      "   5.11228527e-16  1.24790256e-15  5.08088010e-16  6.64935732e-16]\n",
      " [-7.07300473e-15 -2.63545716e-15 -3.60809150e-15  1.00000000e+00\n",
      "  -3.53441202e-15 -1.22768382e-15 -2.29970582e-15 -2.62264867e-16]\n",
      " [ 1.30348743e-14  2.95515821e-15  2.75798559e-15  1.23000406e-14\n",
      "   1.00000000e+00  4.20945500e-15 -9.87233322e-16  4.63996262e-15]\n",
      " [-2.80015751e-15 -1.62318587e-15 -9.97625070e-16 -1.92358684e-15\n",
      "  -1.08689218e-15  1.00000000e+00 -5.90866743e-16 -1.25167019e-15]\n",
      " [-8.88178420e-15 -3.55271368e-15 -3.55271368e-15 -5.32907052e-15\n",
      "  -2.66453526e-15 -1.77635684e-15  1.00000000e+00 -1.33226763e-15]\n",
      " [ 1.24344979e-14  0.00000000e+00  3.55271368e-15  5.32907052e-15\n",
      "   5.32907052e-15  0.00000000e+00  3.55271368e-15  1.00000000e+00]]\n"
     ]
    }
   ],
   "source": [
    "Ia = invA @ A\n",
    "print(Ia)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1., -0., -0., -0., -0., -0., -0.,  0.],\n",
       "       [ 0.,  1.,  0.,  0.,  0.,  0.,  0., -0.],\n",
       "       [ 0.,  0.,  1.,  0.,  0.,  0.,  0.,  0.],\n",
       "       [-0., -0., -0.,  1., -0., -0., -0., -0.],\n",
       "       [ 0.,  0.,  0.,  0.,  1.,  0., -0.,  0.],\n",
       "       [-0., -0., -0., -0., -0.,  1., -0., -0.],\n",
       "       [-0., -0., -0., -0., -0., -0.,  1., -0.],\n",
       "       [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  1.]])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.round(Ia)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vector space "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$v, u \\in \\mathbb{R}^{n}$ and for every $\\alpha \\in \\mathbb{R}^{1}$ we have $u + v \\in \\mathbb{R}^{n}$ and $\\alpha u \\in \\mathbb{R}^{n}$\n",
    "<br>\n",
    "So we have a sum and multiplication on scalar with properties:\n",
    "- for every $u, v, w \\in \\mathbb{R}^{n}$: $(u + v) + w = u + (u + w)$\n",
    "- for every $u, v \\in \\mathbb{R}^{n}$: u + v = v + u\n",
    "- there exists $0 \\in \\mathbb{R}^{n}$ such that: $0 + u = u + 0 = u$\n",
    "- for every $u \\in \\mathbb{R}^{n}$ there exists $-u \\in \\mathbb{R}^{n}$ such that: $u + (-u) = (-u) + u = 0$\n",
    "- for every $\\alpha, \\beta \\in \\mathbb{R}^{1}$ and every $u \\in \\mathbb{R}^{n}$: $\\alpha(\\beta u) = (\\alpha \\beta u)$\n",
    "- for every $u \\in \\mathbb{R}^{n}$: $1u=u1=u$\n",
    "- for every $u, v \\in \\mathbb{R}^{n}$ and $\\alpha \\in \\mathbb{R}^{1}$: $\\alpha (u + v) = \\alpha u + \\alpha v$\n",
    "- for every $\\alpha, \\beta \\in \\mathbb{R}^{1}$ and every $u \\in \\mathbb{R}^{n}$: $(\\alpha + \\beta)u = \\alpha u + \\beta v$\n",
    "<br>\n",
    "So we can define $-$ and $:$ operations as well\n",
    "If some structure satisfies such properties it's called vector space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([0.17466798, 0.93788138, 0.42326657, 0.50182232, 0.59076164]),\n",
       " array([0.36051146, 0.679797  , 0.18116046, 0.11820887, 0.77204126]),\n",
       " 0.4786435074739014,\n",
       " 0.7460982666337203,\n",
       " array([0.35258067, 0.95610619, 0.3377573 , 0.32838943, 0.85878287]))"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "u = np.random.random(5)\n",
    "v = np.random.random(5)\n",
    "x = random.random()\n",
    "y = random.random()\n",
    "w = u * x + v * y\n",
    "u, v, x, y, w\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear combination"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let $v_1, v_2, \\dots v_n \\in \\mathbb{R}^{n}$ and $\\alpha_{1}, \\alpha_{2} \\dots \\alpha_{n} \\in \\mathbb{R}^{1}$ then linear combination of this vectors is called the vector $w = \\sum_{i=1}^{n}\\alpha_{i} v_{i}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vectors $v_1, v_2, \\dots v_n \\in \\mathbb{R}^{n}$ are called linearly independent if none of them can be linear combination of others, of for each $i \\in (1 \\dots n)$ there is no such $\\alpha_{1}, \\alpha_{2} \\dots \\alpha_{i-1}, \\alpha_{i+1} \\dots \\alpha_{n_1} \\in \\mathbb{R}^{1}$ such that $u_i = \\sum_{k = 1, k \\neq i}^{n}\\alpha_{k}u_k $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Maximum amount of linearly independent vectors in vector space is called dimension of this space\n",
    "<br>\n",
    "Every vector space has basis, linearly independent vectors $e_1, e_2 \\dots e_n $ such that every other vector from this space can be achieved by the linear combination of this basis $u = \\sum_{i=1}^n\\alpha_{i}e_i$ and $(\\alpha_{1}, \\alpha_{2}, \\dots, \\alpha_{n})$ are called the coordinates of the vector $u$\n",
    "<br>\n",
    "For $\\mathbb{R}^{n}$ basis is $e_1 = (1, 0, \\dots, 0), e_2 = (0, 1, \\dots, 0), \\dots e_i = (0, 0, \\dots, 1, \\dots, 0), e_n = (0, 0, \\dots, 1)$\n",
    "Or for $\\mathbb{R}^{2}$ basis is $e_1=(1, 0), e_2=(0, 1)$ and for $\\in \\mathbb{R}^{3}$: $e_1=(1, 0, 0), e_2 = (0, 1, 0), e_3=(0, 0, 1)$\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear maps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Map $f:X \\to Y$ between vector spaces $\\mathbb{U}$ and $\\mathbb{V}$ is called linear (or linear transformation) if for every $u, v \\in \\mathbb(U)$ and every scalar $\\alpha in \\in \\mathbb{R}^{1}$ we have:\n",
    "- $f(u + v) = f(u) + f(v)$\n",
    "- $f(\\alpha u) = \\alpha f(u)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let $e_1, e_2, \\dots, e_n$ be a basis for linear space $\\mathbb{U}$ and $l_1, l_2, \\dots, l_m$ basis for $\\mathbb{V}$\n",
    "then $f(e_i) = a_1{li}_1 + a_{2i}l_2 + \\dots + a_{mi}l_m$ for some $a_1, \\dots, a_m \\in \\in \\mathbb{R}^{1}$\n",
    "for we have the following matrix:\n",
    "$$\\begin{align} T &= \\begin{pmatrix}\n",
    "           a_{11}, a_{12}, \\dots, a_{1n} \\\\\n",
    "           a_{21}, a_{22}, \\dots, a_{2n} \\\\\n",
    "           \\vdots \\\\\n",
    "           a_{m1}, a_{n2}, \\dots, a_{mn} \\\\\n",
    "         \\end{pmatrix}\n",
    "  \\end{align}$$\n",
    "<br>\n",
    "which is called the transformation matrix\n",
    "<br>\n",
    "For each $u \\in \\mathbb{U}$ there exists $b_1, b_2, \\dots, b_n \\in \\mathbb{R}^{1}$ such that: $u = b_1e_1 + b_2e_2 + \\dots + b_ne_n$ and for $f(u)$ (from linear property) we have $$f(u) = b_1f(e_1) + b_2f(e_2) + \\dots + b_nf(e_n)$$\n",
    "then from the property - $f(e_i) = a_{1i}l_1 + a_{2i}l_2 + \\dots + a_{mi}l_m$, we get:\n",
    "$$f(u) = Tb$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Eigenvectors and Eigenvalues"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If for some linear transformation $T:\\mathbb{U} \\to \\mathbb{V}$ with transformation matrix $M$, there exists non-zero vector $u \\in \\mathbb{U}$ and scalar $\\lambda \\in \\mathbb{R}^{1}$ such that $T(u) = Mu = \\lambda u$, this vector is called eigenvector for the transformation $T$ and $\\lambda$ is called eigenvalue"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$Mu = \\lambda u$ then $Mu - \\lambda u = 0$ and \n",
    "$$(M - I\\lambda)u = 0$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This equasion has a solution if $|M - I\\lambda|=0$, thus, we can calculate eigenvector and eigenvalue for linear transformation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consider eigenvectors $v_1, v_2, \\dots, v_n$ and eigenvalues $\\lambda_{1}, \\lambda_{2}, \\dots, \\lambda_{n}$ from basis $e_1, e_2, \\dots, e_n$ of transformation matrix $A$, let $Q = (v_1, v_2 \\dots, v_n)$ then:\n",
    "$$AQ = (\\lambda_{1}v_1, \\lambda_{2}v_2, \\dots, \\lambda_{n}v_n)$$\n",
    "define:\n",
    "$$\\begin{align} \\Lambda &= \\begin{pmatrix}\n",
    "           \\lambda_{11}, 0, \\dots, 0 \\\\\n",
    "           0, \\lambda_{22}, \\dots, 0 \\\\\n",
    "           \\vdots \\\\\n",
    "           0, 0, \\dots, \\lambda_{nn} \\\\\n",
    "         \\end{pmatrix}\n",
    "  \\end{align}$$\n",
    "<br>\n",
    "$$AQ = Q\\Lambda$$\n",
    "<br>\n",
    "$$AQQ^{-1}=Q\\Lambda Q^{-1}$$\n",
    "<br>\n",
    "$$AI=Q\\Lambda Q^{-1}$$\n",
    "<br>\n",
    "$$A=Q\\Lambda Q_{-1}$$\n",
    "<br>\n",
    "This is called eigendecomposition of matrix $A$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualisation of Eigenvalues and Eigenvectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we consider transformation matrix A again, and we take bunch of vectors (as much as possible) from given vector \n",
    "space where that matrix is doing transformations, we can find some vectors that never change their orientation but maybe they are scaled by some other factor. \n",
    "\n",
    "Each such vector is called Eigenvector of matrix A which direction isn't affected by transformation, but scale is affected. Scale of that vector will be eigenvalue of that vector.\n",
    "\n",
    "Each eigenvector has its eigenvalue and these can be multiple because of several axis of transformation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Eigenvalues and Eigenvectors example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![SegmentLocal](resources/linear-algebra-02/eigenvalues-and-vectors.gif)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Non-Eigenvalues and Eigenvectors example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![SegmentLocal](resources/linear-algebra-02/non-eigenvalues-and-vectors.gif)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vector (Matrix) norm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$||x||_{p} = \\sqrt[p]{\\sum_{i=1}^{n}x_i^{p}}$$\n",
    "$$||x||_{p} = (\\sum_{i=1}^{n}x_i^{p})^{1/p}$$\n",
    "<br>\n",
    "$L_2$ norm\n",
    "$$||x||_{2} = (\\sum_{i=1}^{n}x_i^{2})^{1/2}$$\n",
    "$L_1$ norm\n",
    "$$||x||_{1} = \\sum_{i=1}^{n}|x_i|$$\n",
    "<br>\n",
    "$$||x||_{\\infty} = max|x_i|$$\n",
    "\n",
    "For matrices Frobenius norm:\n",
    "$$||A||_{F} = (\\sum_{i=1, j=1}^{n, m}a_{ij})^2$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Determinant as a scaling factor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is a link to the [video](https://www.youtube.com/watch?v=Ip3X9LOh2dk) about determinants"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given two vectors x,y in space and some transformation matrix A. \n",
    "If we multiply these vectors by given transformation matrix, we will get transformed vectors. \n",
    "Area value before and after transformation will be changed with exactly the value of determinant of A."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![SegmentLocal](resources/linear-algebra-02/determinant-as-a-scaling-factor.gif)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SVD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For any $A \\in \\mathbb{R}^{n \\times m} (\\mathbb{C}^{n \\times m})$ there exists decomposition:\n",
    "$$A = U \\Sigma V^{T}$$ \n",
    "where $U \\in \\mathbb{R}^{n \\times n}(\\mathbb{C}^{n \\times n})$ is a square matrix, $\\Sigma \\in \\mathbb{R}^{n \\times m} (\\mathbb{C}^{n \\times m})$ is a diagonal matrix and  $V \\in \\mathbb{R}^{n \\times n} (\\mathbb{C}^{n \\times n})$ is also a square matrix\n",
    "#### Note: We only discuss real valued vectors, matrices and tensors in this course"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](resources/linear-algebra-02/singular-value-decomposition-01.jpeg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "short [tutorial](http://web.mit.edu/be.400/www/SVD/Singular_Value_Decomposition.htm) on SVD from MIT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "very interesting medium [blog](https://medium.com/@jonathan_hui/machine-learning-singular-value-decomposition-svd-principal-component-analysis-pca-1d45e885e491) on SVD & PCA "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](resources/linear-algebra-02/singular-value-decomposition-02.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[ 1,  2,  3],\n",
       "        [ 4,  5,  6],\n",
       "        [ 7,  8,  9],\n",
       "        [10, 11, 12]]), (4, 3))"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A = np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9], [10, 11, 12]])\n",
    "A, A.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[-0.14087668,  0.82471435,  0.53358462,  0.12364244],\n",
       "        [-0.34394629,  0.42626394, -0.8036038 ,  0.2328539 ],\n",
       "        [-0.54701591,  0.02781353,  0.00645373, -0.83663514],\n",
       "        [-0.75008553, -0.37063688,  0.26356544,  0.48013879]]),\n",
       " array([2.54624074e+01, 1.29066168e+00, 1.38648772e-15]),\n",
       " array([[-0.50453315, -0.5745157 , -0.64449826],\n",
       "        [-0.76077568, -0.05714052,  0.64649464],\n",
       "        [-0.40824829,  0.81649658, -0.40824829]]),\n",
       " (4, 4),\n",
       " (3,),\n",
       " (3, 3))"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "U, S, V_T = np.linalg.svd(A)\n",
    "U, S, V_T, U.shape, S.shape, V_T.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[2.54624074e+01, 0.00000000e+00, 0.00000000e+00],\n",
       "        [0.00000000e+00, 1.29066168e+00, 0.00000000e+00],\n",
       "        [0.00000000e+00, 0.00000000e+00, 1.38648772e-15]]), (3, 3))"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Sg = np.diag(S)\n",
    "Sg, Sg.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(25.495097567963924, -0.0, 1.0, 1.7320508075688772)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.linalg.norm(A), np.round(U[:, 0] @ U[:, 1].T), np.linalg.norm(U[2, :]), np.linalg.norm(V_T)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Matrix Decompositions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[fastai LA](https://nbviewer.jupyter.org/github/fastai/numerical-linear-algebra/blob/master/nbs/1.%20Why%20are%20we%20here.ipynb#Matrix-Decompositions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[advanced matrix decompositions](https://sites.google.com/site/igorcarron2/matrixfactorizations)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[nfm tutorial](https://perso.telecom-paristech.fr/essid/teach/NMF_tutorial_ICME-2014.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[topic modeling](https://medium.com/@nixalo/comp-linalg-l2-topic-modeling-with-nmf-svd-78c94330d45f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[background removal using svd](https://medium.com/@siavashmortezavi/fast-randomized-svd-singular-value-decomposition-using-pytorch-and-gpus-46b627511a6d)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PCA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Reduction\n",
    "- PCA is most commonly used to condense the information contained in a large number of original variables into a smaller set of new composite dimensions, with a minimum loss of information."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Example](https://www.projectrhea.org/rhea/index.php/PCA_Theory_Examples) of using PCA on image compression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Mapping of 2D points into 1D. \n",
    "PCA Takes the most optimal 1d axis to save data information better, reducing memory by factor of 2. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![SegmentLocal](resources/linear-algebra-02/principal-component-analysis.gif)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interpretation\n",
    "- PCA can be used to discover important features of a large data set. It often reveals relationships that were previously unsuspected, thereby allowing interpretations that would not ordinarily result.\n",
    "PCA is typically used as an intermediate step in data analysis when the number of input variables is otherwise too large for useful analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[example](https://towardsdatascience.com/visualising-high-dimensional-datasets-using-pca-and-t-sne-in-python-8ef87e7915b) usage of pca (and t-SNE) for data visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[example](https://github.com/aviolante/sas-python-work/blob/master/tSneExampleBlogPost.ipynb) notebook for comparing PCA and t-SNE for visualizing MNIST data "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let $X = (x^{1}, x^{2}, \\dots x^{m})$ is our data where $x^{i} = (x_1^{i}, x_2^{i}, \\dots, x_n^{i}) \\in \\mathbb{R}^{n}$ for each $i \\in (1, 2, \\dots, m)$\n",
    "<br>\n",
    "Normalize data with mean $x^{i} = x^{i} - \\frac{1}{m}\\sum_{i=1}^{m}x^{i}$\n",
    "<br>\n",
    "compute the covariance matrix:\n",
    "$$A = \\frac{1}{m} \\sum_{i=1}^{m}(x^{i})(x^{i})^{T}$$\n",
    "<br>\n",
    "take SVD from $X$:\n",
    "$$ A = U\\Sigma V^{T}$$\n",
    "and consider first $k \\leq n$ columns of $U \\in \\mathbb{R}^{n\\times n}$: \n",
    "$$u_1, u_2, \\dots, u_k \\in \\mathbb{R}^{n}$$\n",
    "now consider the matrix $U_{r} = (u_1, u_2, \\dots, u_k)$ and \n",
    "$$z^{i} = U_{r}^{T}x^{i}$$\n",
    "<br>\n",
    "$U_{r}^{T} \\in \\mathbb{R}^{k \\times n}$ and $x^{i} \\in \\mathbb{R}^{n \\times 1}$ thus $z^{i} \\in \\mathbb{R}^{k \\times 1}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reconstruction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can approximate the reconstruction of the original value of $x^{i}$ as \n",
    "$$x_{a}^{i} = U_{r}z^{i}$$\n",
    "<br>\n",
    "$z^{i} \\in \\mathbb{R}^{n \\times 1}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "to check our method we should compare original value to approximation:\n",
    "$$\\frac{\n",
    "\\frac{1}{m}\\sum_{i=1}^{m}||x^{i} - x_{a}^{i}||^{2}\n",
    "}{\n",
    "\\frac{1}{m}\\sum_{i=1}^{m}||x^{i}||^{2}\n",
    "} \\leq \\epsilon$$\n",
    "<br>\n",
    "$\\epsilon$ might be any value, e.g $\\epsilon = 0.01$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\frac{\n",
    "\\frac{1}{m}\\sum_{i=1}^{m}||x^{i} - x_{a}^{i}||^{2}\n",
    "}{\n",
    "\\frac{1}{m}\\sum_{i=1}^{m}||x^{i}||^{2}\n",
    "} \\leq  = 1 -\n",
    "\\frac{\n",
    "\\sum_{i=1}^{k}S_{ii}\n",
    "}{\n",
    "\\sum_{j=1}^{n}S_{jj}\n",
    "}$$\n",
    "<br>\n",
    "So we can calculate\n",
    "$$\\frac{\n",
    "\\sum_{i=1}^{k}S_{ii}\n",
    "}{\n",
    "\\sum_{j=1}^{n}S_{jj}\n",
    "} \\geq \\epsilon$$\n",
    "<br>\n",
    "Only one decomposition is enough"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[pca](https://www.coursera.org/learn/machine-learning/lecture/GBFTt/principal-component-analysis-problem-formulation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Additional Materials"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[jupyter-notebook tips&tricks&shortcuts](https://www.dataquest.io/blog/jupyter-notebook-tips-tricks-shortcuts/)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
